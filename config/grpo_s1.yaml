model:
  # Currently we have three models here
  pretrained_model_path:
    - Qwen/Qwen2.5-3B-Instruct
    - ministral/Ministral-3b-instruct
    - meta-llama/Llama-3.2-3B-Instruct
  device: "cuda"
  dtype: bfloat16
  attn_impl: "sdpa"  
data:
  name: gsm8k
training:
  epochs: 2
  random_seed: 1337
  max_prompt_len: 256
  max_gen_len: 512  # Reduced from 1024 to 512 to save memory
  temperature: 1.0  # Temperature for sampling during generation
  batch_size: 64
  num_questions_per_batch: 32
  num_answers_per_question: 2  # Reduced from 5 to 2 for multi-GPU (total 6 episodes per step)
  # Number of examples per gradient accumulation step
  micro_batch_size: 1  # Reduced from 2 to 1 to save memory
  max_grad_norm: 1.0
  learning_rate: 1.0e-5
  weight_decay: 0.0
  betas: [0.9, 0.999]
  ckpt_dir: "ckpt"
  log_dir: "logs"
  ckpt_save_interval: 50  # More frequent saves since training is faster with multi-GPU
  eval_interval: 10
  memory_efficient_adamw: false