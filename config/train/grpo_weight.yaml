# GRPO Individual Weight Multi-Model Training Configuration
# 基于个体权重的多模型协作训练配置 - 匹配train_weight.py逻辑

model:
  # 预训练模型路径列表 - 支持最多3个模型协作训练
  pretrained_model_path:
    - "Qwen/Qwen2.5-3B-Instruct"
    - "meta-llama/Llama-3.2-3B-Instruct" 
    - "microsoft/Phi-3-mini-128k-instruct"
  
  # 模型精度配置
  dtype: "bfloat16"  # 支持: bfloat16, float16, float32
  
  # FACT适配器配置（可选）
  use_fact: false
  fact_rank: 16
  fact_alpha: 32
  fact_dropout: 0.05
  fact_scale: 1.0
  fact_target_modules: ["q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  # LoRA适配器配置（可选）
  use_lora: false
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

data:
  # 数据集配置
  name: "gsm8k"  # 当前只支持GSM8K数据集

training:
  # 基础训练参数
  epochs: 1
  learning_rate: 0.00001  # 基础学习率，会根据个体权重动态调整
  random_seed: 42
  max_gen_len: 512
  temperature: 1.0  # 匹配代码中的默认值
  
  # GRPO特定参数
  num_answers_per_question: 2  # 每个问题生成的答案数量
  epsilon_low: 0.2   # PPO裁剪下界
  epsilon_high: 0.2  # PPO裁剪上界
  beta: 0.01         # KL散度正则化系数
  loss_type: "grpo"  # 损失函数类型
  
  # 梯度和优化
  max_grad_norm: 1.0
  
  # 参考模型配置
  use_ref_model: true  # 用于KL散度计算，防止模型偏离太远
  
  # ===== 个体权重配置 (Individual Weights) =====
  # 基于sigmoid函数的个体权重计算: weight = 1 / (1 + exp(k * (reward - threshold)))
  individual_weights:
    k: 5.0           # sigmoid函数的陡峭度参数，值越大变化越陡峭
    threshold: 0.5   # 奖励阈值，高于此值权重变小，低于此值权重变大
  
  # ===== 排名权重配置 (备用，代码中未使用) =====
  ranking_weights:
    min_weight: 0.3    # 表现最好模型的最小权重
    max_weight: 2.0    # 表现最差模型的最大权重
  
  # 检查点保存
  ckpt_save_interval: 3000  # 
  ckpt_dir: "checkpoints/grpo_weights"

# ===== 配置说明 =====
# 个体权重机制说明：
# - k=5.0, threshold=0.5: 奖励0.5以上的模型权重快速下降，奖励0.5以下的权重快速上升
# - k值越大，权重变化越陡峭
# - threshold是权重变化的中心点
# 
# 多模型协作机制：
# - 每个模型生成自己的episodes
# - 同时学习其他模型的高质量episodes（通过重新tokenize）
# - 根据性能动态调整学习率 (lr = base_lr * weight)
# 
# 内存优化：
# - 每5步进行积极的内存清理
# - 每10步显示GPU内存使用情况
# - 支持最多3个模型（受GPU数量限制） 