 # GRPO Threshold-based Multi-Model Training Configuration
# 基于阈值的多模型协作训练配置

model:
  # 预训练模型路径列表 - 支持多个模型协作训练
  pretrained_model_path:
    - "Qwen/Qwen2.5-3B-Instruct"
    - "meta-llama/Llama-3.2-3B-Instruct" 
    - "microsoft/Phi-3-mini-128k-instruct"
  
  # 模型精度配置
  dtype: "bfloat16"  # 支持: bfloat16, float16, float32
  
  # 梯度检查点配置 - 节省显存但会增加计算时间
  gradient_checkpointing: true  # 开启梯度检查点以节省显存
  
  # FACT适配器配置（可选）
  use_fact: false
  fact_rank: 16
  fact_alpha: 32
  fact_dropout: 0.05
  fact_scale: 1.0
  fact_target_modules: ["q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  # LoRA适配器配置（可选）
  use_lora: false
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules: ["q_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

data:
  # 数据集配置
  name: "gsm8k"  # 当前只支持GSM8K数据集

training:
  # 基础训练参数
  epochs: 1
  learning_rate: 0.00001
  random_seed: 42
  max_gen_len: 512
  temperature: 0.7
  
  # GRPO特定参数
  num_answers_per_question: 4  # 每个问题生成的答案数量
  epsilon_low: 0.2   # PPO裁剪下界
  epsilon_high: 0.2  # PPO裁剪上界
  beta: 0.01         # KL散度正则化系数
  loss_type: "grpo"  # 损失函数类型
  
  # 梯度和优化
  max_grad_norm: 1.0
  
  # 参考模型配置
  use_ref_model: true
  
  # ===== THRESHOLD配置 =====
  # 质量阈值 - 只有reward达到此阈值的其他模型答案才会被当前模型学习
  reward_threshold: 1.0  # 设置为1.0表示只学习完全正确的答案
  
  # 检查点保存
  ckpt_save_interval: 3000
  ckpt_dir: "checkpoints/grpo_threshold"

# 说明：
# - reward_threshold=1.0: 只学习reward=1.0的完全正确答案
# - reward_threshold=0.8: 学习reward>=0.8的高质量答案
# - reward_threshold=0.0: 学习所有答案（等同于不使用threshold）
# 
# 注意：使用threshold可能导致某些更新步骤的样本数少于num_answers_per_question，
# 这是正常现象，因为低质量答案被过滤掉了。 